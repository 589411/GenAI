{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qeuxx1-y8r1h"
      },
      "source": [
        "# Addictive Learning with langchain\n",
        "\n",
        "# Overview ðŸ”Ž\n",
        "\n",
        "The tutorial demonstrates how to generate addictive video generation like reels in instagram by using Langchain for script generation, Eleven labs for text to speech, Assembly AI to generate subtitles and moviepy to add speech and subtitles to video\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Modern apps like YouTube and Instagram have shortened attention spans, making it harder for people to engage with text-heavy content like PDFs. This project uses AI to extract interesting facts from PDFs and presents them in short, engaging videos, such as Minecraft parkour, making information more accessible and captivating.\n",
        "\n",
        "\n",
        "## Key Components\n",
        "- OpenAI's GPT: To Extract facts from pdf and generate the script for video\n",
        "- Eleven Labs: To Generate audio from the script using Text to speech\n",
        "- Assembly AI: To create subtitles from audio\n",
        "- MoviePy: To integrate audio and subtitles with a base background video\n",
        "\n",
        "## Implementation\n",
        " 1. **PDF Text Extraction**: The project begins by extracting text from the provided PDF using the pypdf library. This ensures all textual content is consolidated into a single string for further processing.\n",
        " 2. **AI-Powered Fact Generation**: OpenAI's GPT model is used to extract interesting and specific facts from the PDF content. A structured prompt ensures the generated output aligns with the project's goals.\n",
        " 3. **Video Script Generation**: Using the extracted facts, the system creates engaging scripts for short videos. These scripts are designed to capture attention quickly, such as by using hooks or interesting questions.\n",
        " 4. **Video Creation**: The moviepy library is used to combine the script with Minecraft parkour visuals, ensuring the videos are both engaging and educational\n",
        " 5. **Audio generation**: Elevenlabs will be used to generate audio from our script using text-to-speech API\n",
        " 6. **Subtitles genearation**: Assembly AI will be used to generate the subtitles from audio\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This project demonstrates the use of AI to transform dense and complex PDF content into engaging, bite-sized video formats. By leveraging modern tools like OpenAI's language models and creative visuals, it bridges the gap between traditional text-based information and the fast-paced, visually-driven preferences of today's audiences. The result is a powerful tool that not only makes information more accessible but also promotes learning in a way that's fun and aligned with modern attention spans.\n",
        "\n",
        "Future improvements could include adding customization options for video themes or expanding support for more content types, ensuring the tool continues to evolve with user needs.\n",
        "\n",
        "![Reel Agent](../images/reel_agent.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeT3UXvCAtGN"
      },
      "source": [
        "### Install and import the necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsbgfX72L6Ez"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_openai langchain_core pypdf moviepy assemblyai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq_Nlfec59lp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pypdf import PdfReader\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import requests\n",
        "import urllib\n",
        "import time\n",
        "import assemblyai as aai\n",
        "from moviepy.editor import *\n",
        "from moviepy.editor import VideoFileClip\n",
        "from moviepy.video.fx import crop\n",
        "from moviepy.video.tools.subtitles import SubtitlesClip\n",
        "from moviepy.config import change_settings\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "import os\n",
        "import random\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bULWDdWwAfzk"
      },
      "source": [
        "### Querying PDF\n",
        "\n",
        "We download a pdf and use OpenAI to extract interesting facts from the PDF. You can use your any PDF you want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_lxn3u459lu",
        "outputId": "0111f993-4e51-4b25-e06f-e550517bf22a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-18 10:26:54--  https://arxiv.org/pdf/1706.03762\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2215244 (2.1M) [application/pdf]\n",
            "Saving to: â€˜1706.03762â€™\n",
            "\n",
            "1706.03762          100%[===================>]   2.11M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-11-18 10:26:54 (27.9 MB/s) - â€˜1706.03762â€™ saved [2215244/2215244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!wget https://arxiv.org/pdf/1706.03762\n",
        "!mv 1706.03762 attention_is_all_you_need.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB44p_lPBn1f"
      },
      "source": [
        "### Extract Facts and Generate script for video\n",
        "\n",
        "Now we extract intresting facts from the PDF that user provides using OpenAI.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5UiYppz59lw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Read text from the pdf\n",
        "def read_pdf():\n",
        "    reader = PdfReader(\"attention_is_all_you_need.pdf\")\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# get facts from the text using OpenAI\n",
        "def get_facts(text):\n",
        "    facts_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        You are a research agent. You are given this information {information}. Your goal is to boil down to interesting and specific insights from this information.\n",
        "\n",
        "        1. Interesting: Insights that people will find surprising or non-obvious.\n",
        "\n",
        "        2. Specific: Insights that avoid generalities and include specific examples from the expert. Here is your topic of focus and set of goals.\n",
        "\n",
        "        3. Provide your answer in points\n",
        "\n",
        "        4. Do not make up your answer on your own and use the information that is provided to you.\n",
        "    \"\"\")\n",
        "\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "    )\n",
        "\n",
        "    chain = facts_prompt | llm\n",
        "    return chain.invoke({\"information\": text}).content\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSYG74lSqaBr"
      },
      "source": [
        "### Creating Script For the Video\n",
        "\n",
        "we then generate the script for audio using the facts that we extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrxZeq1nqZAU"
      },
      "outputs": [],
      "source": [
        "class Script(BaseModel):\n",
        "    script: str = Field(description=\"script for the video\")\n",
        "    title: str = Field(description=\"title of the video\")\n",
        "    description: str = Field(description=\"description of the video\")\n",
        "    keywords: list[str] = Field(description=\"keywords for the video\")\n",
        "\n",
        "class Scripts(BaseModel):\n",
        "    scripts: list[Script]\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=Scripts)\n",
        "\n",
        "# create scripts for the reel using the facts\n",
        "def create_scripts(facts):\n",
        "    script_prompt = PromptTemplate(template = \"\"\"\n",
        "    .\\n{format_instructions}\\\n",
        "    You are an expert script writer. You are tasked with writing scripts for 20-second video that plays on YouTube. Given these facts {facts} you need to write five engaging scripts keeping these facts in the context of the script.\n",
        "    keep in mind\n",
        "\n",
        "    1. Your scripts should not sound monotonous.\n",
        "\n",
        "    2. Each script should start with an engaging pitch that hooks viewers to watch the entire video. for example, you can use a fact or a question at the start of the video.\n",
        "   \"\"\",\n",
        "    input_variables=[\"facts\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "    )\n",
        "    chain = script_prompt | llm | parser\n",
        "\n",
        "    return chain.invoke({\"facts\": facts})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGdO71p_59ly"
      },
      "outputs": [],
      "source": [
        "# read text from pdf\n",
        "text = read_pdf()\n",
        "\n",
        "# find facts from the pdf\n",
        "facts = get_facts(text)\n",
        "\n",
        "# Generate scripts for the reel using the facts\n",
        "scripts = create_scripts(facts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "111h8zh259l0"
      },
      "source": [
        "### Create audio file\n",
        "\n",
        "We will now create the audio from the script using text-to-speech API provided by eleven labs. You can get your eleven labs API Key from [Eleven labs](https://elevenlabs.io/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpzoNcc459l4"
      },
      "outputs": [],
      "source": [
        "voices_data = {\n",
        "      \"name\": \"Rachel\",\n",
        "      \"description\": \"A smooth and natural voice ideal for conversational and professional use cases.\",\n",
        "      \"voice_id\": \"21m00Tcm4TlvDq8ikWAM\",\n",
        "      \"voice_settings\": {\n",
        "        \"pitch\": 1.0,\n",
        "        \"speed\": 1.3,\n",
        "        \"intonation\": \"balanced\",\n",
        "        \"clarity\": \"high\",\n",
        "        \"volume\": \"normal\"\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzhdGfA859l6",
        "outputId": "bcb65443-5b57-492b-9f33-327c7a3b72f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio stream saved successfully to output.mp3.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "CHUNK_SIZE = 1024\n",
        "XI_API_KEY = userdata.get('XI_API_KEY')\n",
        "\n",
        "VOICE_ID = voices_data[\"voice_id\"]\n",
        "TEXT_TO_SPEAK = scripts[\"scripts\"][0][\"script\"]\n",
        "voice_settings = voices_data[\"voice_settings\"]\n",
        "\n",
        "tts_url = f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream\"\n",
        "headers = {\n",
        "    \"Accept\": \"application/json\",\n",
        "    \"xi-api-key\": XI_API_KEY\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"text\": TEXT_TO_SPEAK,\n",
        "    \"model_id\": \"eleven_multilingual_v2\",\n",
        "    \"voice_settings\": {\n",
        "        \"stability\": 0.5,\n",
        "        \"similarity_boost\": 0.8,\n",
        "        \"style\": 0.0,\n",
        "        \"use_speaker_boost\": True,\n",
        "        **voice_settings\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(tts_url, headers=headers, json=data, stream=True)\n",
        "\n",
        "# Handle response\n",
        "\n",
        "if response.ok:\n",
        "    output_path = f\"output.mp3\"\n",
        "    with open(output_path, \"wb\") as audio_file:\n",
        "        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
        "            audio_file.write(chunk)\n",
        "    print(f\"Audio stream saved successfully to {output_path}.\")\n",
        "else:\n",
        "    print(f\"Error {response.status_code}: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rESeVBB59l8"
      },
      "source": [
        "### Integrate audio with video using moviepy\n",
        "\n",
        "We will add audio to any attention grabbing background video using moviepy. I am using this [Video](https://drive.google.com/file/d/14kiCtrgoCwJzcdYFmZP3FKhoiUBFgVrv/view?usp=sharing) and we will also add a quiet background [music](https://drive.google.com/file/d/1mWBAD2b-vj3HZayAVeVk2PGsAYZDJLpf/view?usp=sharing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2-67j2x59l9",
        "outputId": "8569c7b3-c689-4fe8-8948-41df98e5d163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Building video movie.mp4.\n",
            "MoviePy - Writing audio in movieTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video movie.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready movie.mp4\n"
          ]
        }
      ],
      "source": [
        "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips,CompositeAudioClip\n",
        "from moviepy.audio.fx.all import audio_loop\n",
        "video = VideoFileClip(\"./videos/videoplayback.mp4\")\n",
        "audio = AudioFileClip(\"./output.mp3\")\n",
        "music = AudioFileClip(\"./music/music1.mp3\")\n",
        "video_loops = int(audio.duration // video.duration) + 1\n",
        "video = concatenate_videoclips([video] * video_loops).subclip(0, audio.duration)\n",
        "music = audio_loop(music, duration=video.duration)\n",
        "music = music.volumex(0.1)\n",
        "audio_music = CompositeAudioClip([music,audio])\n",
        "final_video = video.set_audio(audio_music)\n",
        "final_video.write_videofile(\"movie.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBb_ray_59mB"
      },
      "source": [
        "# Generate subtitles using Assembly AI\n",
        "\n",
        "We will also add subtitles to the video. we will be using [Assembly AI](https://www.assemblyai.com) to generate our subtitles. You will need an API key that you can get by registering in the website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar4KmGJo59mC",
        "outputId": "a943df34-deea-48a4-aeab-aabe35c89db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "00:00:00,360 --> 00:00:01,016\n",
            "Ever wondered how\n",
            "\n",
            "2\n",
            "00:00:01,516 --> 00:00:01,721\n",
            "Transformers beat\n",
            "\n",
            "3\n",
            "00:00:01,753 --> 00:00:02,825\n",
            "RNNs at their own\n",
            "\n",
            "4\n",
            "00:00:02,865 --> 00:00:04,369\n",
            "game? By ditching\n",
            "\n",
            "5\n",
            "00:00:04,417 --> 00:00:04,889\n",
            "sequential\n",
            "\n",
            "6\n",
            "00:00:04,937 --> 00:00:05,873\n",
            "computation for a\n",
            "\n",
            "7\n",
            "00:00:05,889 --> 00:00:06,417\n",
            "cutting edge\n",
            "\n",
            "8\n",
            "00:00:06,441 --> 00:00:07,585\n",
            "attention mechanism,\n",
            "\n",
            "9\n",
            "00:00:07,745 --> 00:00:08,817\n",
            "Transformers train\n",
            "\n",
            "10\n",
            "00:00:08,881 --> 00:00:10,340\n",
            "faster. Just 12\n",
            "\n",
            "11\n",
            "00:00:10,445 --> 00:00:12,505\n",
            "hours on 8B100 GPUs\n",
            "\n",
            "12\n",
            "00:00:12,665 --> 00:00:13,465\n",
            "ready for a deep\n",
            "\n",
            "13\n",
            "00:00:13,505 --> 00:00:13,985\n",
            "learning upgrade.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "aai.settings.api_key =  userdata.get(\"ASSEMBLYAI_API_KEY\")\n",
        "\n",
        "# create subtitles from audio file using asssemby AI\n",
        "\n",
        "FILE_URL = \"./output.mp3\"\n",
        "transcriber = aai.Transcriber()\n",
        "transcript = transcriber.transcribe(FILE_URL)\n",
        "subtitles = transcript.export_subtitles_srt(20)\n",
        "f = open(\"video.srt\",\"w\")\n",
        "f.write(subtitles)\n",
        "if transcript.status == aai.TranscriptStatus.error:\n",
        "    print(transcript.error)\n",
        "else:\n",
        "    print(subtitles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhhEaLUo59mD"
      },
      "source": [
        "### Add subtitles to our video\n",
        "Once our subtitles are generated, we add the subtitles to video using moviepy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smjo_gVAuPHx"
      },
      "source": [
        "We need to install Imagemagick to add subtitles to our video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7ptl8hlhXRun"
      },
      "outputs": [],
      "source": [
        "!apt update &> /dev/null\n",
        "!apt install imagemagick &> /dev/null\n",
        "!apt install ffmpeg &> /dev/null\n",
        "!pip3 install moviepy[optional] &> /dev/null\n",
        "!sed -i '/<policy domain=\"path\" rights=\"none\" pattern=\"@\\*\"/d' /etc/ImageMagick-6/policy.xml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hadyIx6HzrRC",
        "outputId": "d87b3a16-09fd-4714-d241-ff329f9ddd21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-18 10:29:49--  https://gist.githubusercontent.com/Kaif987/38fca3821fbbcbd7b60cb54df348c2e8/raw/7745747309ffb1982467b138d07f6f2405a5da34/policy.xml\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7947 (7.8K) [text/plain]\n",
            "Saving to: â€˜policy.xmlâ€™\n",
            "\n",
            "\rpolicy.xml            0%[                    ]       0  --.-KB/s               \rpolicy.xml          100%[===================>]   7.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-18 10:29:49 (80.8 MB/s) - â€˜policy.xmlâ€™ saved [7947/7947]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/Kaif987/38fca3821fbbcbd7b60cb54df348c2e8/raw/7745747309ffb1982467b138d07f6f2405a5da34/policy.xml\n",
        "!mv policy.xml /etc/ImageMagick-6/policy.xml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKTmYAMUunfB"
      },
      "source": [
        "Here we are installing the Impact font that will be used in subtitles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj5x1ZuG9ull",
        "outputId": "3be29f34-4267-4a6b-be31-cb14b6ad4bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-18 10:29:49--  https://github.com/sophilabs/macgifer/blob/master/static/font/impact.ttf\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜Impact.ttfâ€™\n",
            "\n",
            "Impact.ttf              [ <=>                ] 285.73K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-11-18 10:29:49 (2.78 MB/s) - â€˜Impact.ttfâ€™ saved [292587]\n",
            "\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 5 dirs\n",
            "/usr/share/fonts/cMap: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/usr/share/fonts/cmap: caching, new cache contents: 0 fonts, 5 dirs\n",
            "/usr/share/fonts/cmap/adobe-cns1: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/usr/share/fonts/cmap/adobe-gb1: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/usr/share/fonts/cmap/adobe-japan1: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/usr/share/fonts/cmap/adobe-japan2: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/usr/share/fonts/cmap/adobe-korea1: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/usr/share/fonts/opentype: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/opentype/urw-base35: caching, new cache contents: 35 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 4 dirs\n",
            "/usr/share/fonts/truetype/droid: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/noto: caching, new cache contents: 3 fonts, 0 dirs\n",
            "/usr/share/fonts/type1: caching, new cache contents: 0 fonts, 2 dirs\n",
            "/usr/share/fonts/type1/gsfonts: caching, new cache contents: 35 fonts, 0 dirs\n",
            "/usr/share/fonts/type1/urw-base35: caching, new cache contents: 35 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/usr/share/fonts/cMap: skipping, looped directory detected\n",
            "/usr/share/fonts/cmap: skipping, looped directory detected\n",
            "/usr/share/fonts/opentype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/type1: skipping, looped directory detected\n",
            "/usr/share/fonts/cmap/adobe-cns1: skipping, looped directory detected\n",
            "/usr/share/fonts/cmap/adobe-gb1: skipping, looped directory detected\n",
            "/usr/share/fonts/cmap/adobe-japan1: skipping, looped directory detected\n",
            "/usr/share/fonts/cmap/adobe-japan2: skipping, looped directory detected\n",
            "/usr/share/fonts/cmap/adobe-korea1: skipping, looped directory detected\n",
            "/usr/share/fonts/opentype/urw-base35: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/droid: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/noto: skipping, looped directory detected\n",
            "/usr/share/fonts/type1/gsfonts: skipping, looped directory detected\n",
            "/usr/share/fonts/type1/urw-base35: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!wget -O Impact.ttf \"https://github.com/sophilabs/macgifer/blob/master/static/font/impact.ttf\"\n",
        "!mkdir -p ~/.fonts\n",
        "!mv Impact.ttf ~/.fonts/\n",
        "!fc-cache -f -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NVrUoD-u14M"
      },
      "source": [
        "Finally we add the subtitles to our video using moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP3aBT3i59mE",
        "outputId": "e4562860-40f4-469e-a007-72e457c07d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: os.environ[IMAGEMAGICK_BINARY]: command not found\n",
            "Moviepy - Building video final.mp4.\n",
            "MoviePy - Writing audio in finalTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video final.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 872/873 [01:25<00:00, 14.24it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file movie.mp4, 2764800 bytes wanted but 0 bytes read,at frame 872/873, at time 14.53/14.54 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready final.mp4\n"
          ]
        }
      ],
      "source": [
        "!os.environ[\"IMAGEMAGICK_BINARY\"] = \"/usr/bin/convert\"\n",
        "change_settings({\"IMAGEMAGICK_BINARY\": r\"/usr/bin/convert\"})\n",
        "generator = lambda txt: TextClip(txt, font='Impact', fontsize=50, color='white',stroke_color=\"black\",stroke_width=1)\n",
        "subtitles = SubtitlesClip(\"video.srt\", generator)\n",
        "video = VideoFileClip(\"movie.mp4\")\n",
        "result = CompositeVideoClip([video, subtitles.set_pos(('center'))])\n",
        "result.write_videofile(\"final.mp4\", fps=video.fps, remove_temp=True, codec=\"libx264\", audio_codec=\"aac\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaGQqtOtWZHR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
